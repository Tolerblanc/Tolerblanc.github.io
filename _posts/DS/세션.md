## 들어가며

- 작년에 병묵님께서 발표하신 엔데 문서 발견!
    - 링크 (대충 Transformer와 Nvidia, 병렬처리에 대한 내용들)
- 이 내용을 확장하여 좀 더 넓은 관점에서, 언어모델-specific한 이야기들을 다뤄보려고 합니다.

<aside>
⭕

이런 걸 다뤄보려고 합니다

- 지금의 LLM에 도달하기 까지 `언어 모델`의 진화 과정
- LLM의 동작 원리, 왜 이런 구조가 주류가 되었는지
- LLM을 더 잘 이해하고 트렌드를 캐치하기 위한 키워드들 (파라미터, 모델 크기, 학습, Reasoning 등)
</aside>

<aside>
❌

이런건 다루지 않습니다

- 각 모델 아키텍처의 상세 구조
- 좋은 결과를 얻기 위한 프롬프트 작성법
- 
</aside>

<aside>
💡

LLM까지의 발전 과정을 간략하게 `어떤 문제를 어떻게 해결했는지` 를 중점으로 알아보고

왜 AI를 믿을 수 없다고 하는지(할루시네이션이 왜 생기는지)

왜 RAG나 MCP 같은 것들이 생겨났는지 등을 정리합니다.

</aside>

## LLM(Large Language Model) 이 등장하기까지

### Prerequisite: ML (Machine Learning) 과 DL (Deep Learning)

- 대부분의 소프트웨어는 규칙기반 (Rule-based)
- 내일 날씨를 예측하는 프로그램을 짜야한다고 생각해봅시다.
    - 날씨에 영향을 주는 다양한 조건을 마련하여 내일 날씨를 결정하면 되겠죠?
    - 오늘이 6-8월 장마철인지?
    - 오늘 구름이 많았는지? 일부가 먹구름이었는지?
    - 옆집 할머니의 무릎이 아픈지?
    - 어떤 변수가 얼마나 영향을 주는지, 전부 체크하여 대응하는 것은 사실상 불가능에 가깝다
- 컴퓨터를 인간처럼 학습시켜 **스스로 규칙을 형성**할 수 있지 않을까?
    - “사람이 일일이 규칙을 쓰기 힘들다” → 데이터를 주고 **규칙은 모델이 찾게 하자**
    - 결국 *입력(Features)* → *출력(Label)* 관계를 **함수**로 근사하는 문제
    - 데이터셋($D = \{X, y\}$)이 충분하다면, 이 $X$ 와 $y$ 간 관계를 나타내는 함수 $f$를 찾아낼 수 있다.
        - $X$: 오늘 습도, 온도, 구름량 등 결과와 상관관계가 있다고 생각되는 변수들
        - $y$ : 다음 날 실제 비가 왔는지에 대한 여부
        - $y$ 가 있냐에 따라서 지도학습과 비지도학습으로 나눌 수 있음
    - $f = a_1 X_1 + a_2 X_2 + … + b$ 가 강수 여부(분류)가 될 수도, 강수 확률(회귀)이 될 수도 있다.
    - $a_1, a_2, … b$ 를 **파라미터(Parameter) 또는 가중치 (Weight)** 라고 하며, 컴퓨터가 데이터를 통해 학습한 규칙으로 볼 수 있다. ⇒ 즉, 파라미터는 `학습` 이라는 과정을 통해 얻어지는 숫자 덩어리
        - 정확히 $b$ 는 편향(bias)이라는 값이며, $f$ 의 최종 값을 조절하는 역할을 함
- 하지만 현실은 그리 만만하지 않다.
    - ML도 충분히 잘 동작하지만, 매우 복잡한 문제(선형함수로 근사될 수 없는)에는 약하다.
    - 예) XOR 문제는 ML로 절대 해결할 수 없는 문제 중 하나
        
        ![image.png](attachment:3735a048-bcbf-48ad-8ef8-2a03fb643080:image.png)
        
    - 하지만 함수 $f$를 여러번 합성하는 형태로 모델링하면, 이런 복잡한 문제도 풀어낼 수 있다.
    - ML에서 특징을 잡기 위해 했던 계산들을, 오차계산과 역계산을 통해 조금씩 수정하여 좀 더 정확한 윤곽을 얻는게 DL
    - 그래서 DL에는 은닉 층(Hidden Layer)이 3개 이상 들어가며, 입력과 출력 사이에 위치하게 된다.
        - 이 은닉층이 깊어질수록, 어느 정도까지는 성능이 매우 좋아짐 → DL
        
        ![동작 형태가 뉴런과 비슷해서 (활성화함수를 통해 은닉층에서 일부만 활성화) 인공신경망(Neural Network)이라고 함](attachment:3668db18-db31-48a8-8690-98f6ffba8214:image.png)
        
        동작 형태가 뉴런과 비슷해서 (활성화함수를 통해 은닉층에서 일부만 활성화) 인공신경망(Neural Network)이라고 함
        
        - 선 하나하나와 은닉층 전부가 파라미터라서 이를 전부 구하려면 압도적인 데이터와 계산이 필요했기 때문에 예전에는 주목받지 못하는 이론이었지만
        - 이를 타파하기 위한 다양한 계산법들이 개발되고 GPU를 통한 병렬계산이 발전하면서 주류로 떠오르게됨
- 그래서 DNN (Deep Neural Network) 의 학습 과정은? (수식 주의)
    - **순전파(Forward)**
        
        모델이 입력 $x$ 에 대해 예측값을 계산합니다.
        
        - $\hat{y}=f_\theta(x)$
    - **손실 계산**
        
        모델 예측 $\hat{y}$ 과 실제 정답 $y$ 사이의 오차를 손실 함수로 측정합니다.
        
        - 예: 평균 제곱 오차(MSE)
            - $\displaystyle \mathcal{L}(\theta)=\tfrac12(y-\hat{y})^{2}$
    - **그래디언트(Gradient) 계산**
        
        손실 $\mathcal{L}$ 을 각 파라미터 $\theta$ 에 대해 미분하여 기울기를 구합니다.
        
        - $\displaystyle \nabla_\theta \mathcal{L}(\theta)=\frac{\partial \mathcal{L}}{\partial \theta}$
    - **가중치 업데이트(Gradient Descent)**
        
        학습률 $\eta$ 만큼 그래디언트의 *반대 방향*으로 파라미터를 조정합니다.
        
        - $\displaystyle \theta ;\leftarrow; \theta - \eta ,\nabla_\theta \mathcal{L}(\theta)$
    - **반복(Iterate)**
        
        손실이 충분히 작아질 때까지 **1 → 4** 단계를 여러 에포크(epoch) 반복합니다.
        
    - 그래디언트의 **반대 방향**으로 파라미터를 조정하여 계산을 최적화하는 방식을 역전파(Back Propagation) 라고 함

### RNN(Recurrent Neural Network) 의 등장과 Seq2Seq

- 완벽할 줄 알았던 딥러닝도 큰 단점과 한계점이 있었는데…
    - 은닉층을 때려박으면 세상 모든 문제든 풀 수 있을 줄 알았지만, 깊어질수록 초기 입력 정보가 사라진다. → 긴 흐름이 필요한 언어 / 음성 등 분야와 시계열 데이터에는 활용이 불가능한 수준
    - 입력 크기가 무조건 고정되어 있어야 함
    → 고정된 크기의 이미지 등은 적합하지만, 길이가 가변이고 순서 의존적인 텍스트 등은 불리함
    - 학습을 위해 고수준으로 정제된 데이터셋을 요구
    → 반드시 사람의 손길을 타야만 함
- 그래서 시간축 개념을 도입하여, 같은 가중치 묶음을 시간 축에 공유하여 한 칸씩 순환하여 처리하는 구조를 도입 ⇒ RNN

![image.png](attachment:8b57031e-ec7a-468a-97de-fa8b2001030b:image.png)

- RNN에서 나와 다시 들어가는 것이 시간축에 묶이는 은닉층
    - 즉, RNN은 특정 시간대의 정보를 하나의 벡터로 요약함 (이전까지 본 내용의 요약)
- RNN은 언어 / 음성 / 시계열 데이터에서 처음으로 실용적인 예측을 해냈지만…
    - 위 사진에서 볼 수 있는 것 처럼, 입력 벡터 $x$ 가 들어가면 출력 벡터 $y$ **하나**가 나오는 구조
        - 토큰 (→ 이 때는 보통 단어) 하나가 벡터 하나에 매칭되고
        - 출력 벡터를 다시 토큰에 매칭시켜 결과 출력
        - 이를 임베딩(embedding) 이라고 함
    - 어느정도 맥락을 기억하기 시작했지만, 장기 의존성이 필요한 작업에는 여전히 약함
    - 구조 상 앞부분의 맥락만 참고 가능 → 뒷 부분을 참고할 수 있도록 하는 메커니즘도 도입된 적 있지만, 계산량이 2배
    - 순차적으로 처리하는 구조라 병렬처리 불가능 → 학습과 추론 둘 다 너무 느림
    - 그래서 LSTM(Long Short-Term Memory)과 같이 어떤 정보를 기억할지 잊어버릴지를 제어할 수 있는 게이트 개념이 들어간 구조도 고안되었음 → 속도 병목은 그대로

- 하나의 은닉 벡터에 꾸겨 넣는게 문제라면, 은닉 벡터를 가지고 다시 생성하는 친구가 있으면 되는거 아닌가?
    - 이게 바로 Seq2Seq 구조
    - 2개의 RNN(LSTM)을 사용한다.
        - Encoder RNN이 특정 입력을 하나의 벡터로 출력하면
        - 이 벡터를 가지고 Decoder RNN이 출력 벡터(Context)를 생성
        - Decoder RNN이 처음 받는 벡터를 시드로 잡고, 그 때 확률이 가장 높은 Output Vector를 생성하는 방식
    
    [seq2seq_4.mp4](attachment:f6f1fdae-8417-497b-9b5a-0789d731079e:seq2seq_4.mp4)
    

- Seq2Seq는 사상 최초로 엔드투엔드 번역에 성공한 구조
    - 근데 아직 느린건 여전하다.
    - 근본 구조는 RNN이라 여전히 문제점을 갖는다.
    → 하나의 벡터에 정보가 압축되므로, 긴 입력은 처리가 불가능하다. (갈수록 잊혀짐)
    → 순차 처리가 필수라서 지연이 길다

### Attention 과 Transformer

- 갈수록 잊혀지는 정보들을 어떻게 살릴 수 있을까?
    - RNN 특징을 다시 보면 → **RNN은 특정 시간대의 정보를 하나의 벡터로 요약함**
    - 지금까지는 요약되는 Context 벡터를 계속 덮어씌웠는데, 이거를 다 가져와서 때려박으면 되지 않을까?
    - 아래와 같이 Encoder RNN에서 얻을 수 있는 Context 벡터를 전부 살려서, Decoder RNN이 각 입력 토큰 중 **어디에 집중할 지에 대한 가중치**를 계산하도록 함
    
    [seq2seq_7.mp4](attachment:e9092989-aa81-4fb3-91a1-946ace592c5d:seq2seq_7.mp4)
    
- 상세한 원리가 궁금하신 분들만 열어보는 걸로…
    - Seq2Seq의 디코더가 각 스텝마다 가장 관련성이 높은 Input 토큰에 집중하게 되는 메커니즘
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/33c92461-742a-4986-aef2-4db8e6ac2b99/Untitled.png)
    
    1. Attention Decoder RNN은 <END> 와 $h_{init}$(initial decoder hidden state)를 입력받는다.
    2. 두 개의 입력을 가지고 새로운 Hidden State($h_4$)를 출력한다. RNN 자체의 출력은 버려진다.
    3. Context Vector($C_4$)를 생성하고, $h_4$와 이어붙인다. (concatenate)
        1. Decoder의 모든 Hidden State 마다 Attention Score를 계산한다.
            
            ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/9d9127d2-f97d-4665-bfb2-f8abfb62060f/Untitled.png)
            
        2. Attention Score를 Softmax하고, 결과값을 가중치로 하여 Decoder Hidden State와 곱한다.
        3. b의 결과를 더하여 Context Vector를 생성한다.
    4. 이어붙인 벡터를 FFNN(FeedForward Neural Networks; seq2seq 모델 내에서 함께 학습되는 layer)에 통과시킨다.
    5. FFNN의 출력은 현재 스텝의 출력 단어를 나타낸다. 위 과정을 <eos>까지 반복한다.
    - Attention 가중치를 활용하여 각 출력이 어떤 입력 정보를 참고하였는지 확인 가능하다. (Explainable)
    - Image2Text 등 멀티모달 데이터에 활용할 수 있다.
        
        ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/c8e65e83-46df-40d3-acfb-849cda999a47/Untitled.png)
        
- Attention 메커니즘은 당시에 폭발적인 관심을 받았다.
    - 당시 자연어 처리에서 다루던 거의 모든 태스크에 대한 기록을 갈아치웠다.
    - 하지만 사용되는 베이스 모델이 RNN이라는 것이 문제였는데,
    - 고질적인 RNN의 문제점을 그대로 답습한다.
    → 순차 처리가 필수라 느린 속도, 병렬처리 불가
    → 여전히 장기 의존성에 약함
    → 이전 맥락만 참조 가능
- Attention Is All You Need
    - 2017년 갑자기 등장한 전설의 논문 “Attention Is All You Need” (by Google)
    - 제목 그대로, RNN을 갖다 버리고 Attention 메커니즘만 취하는 모델인 Transformer를 발표
    
    ![image.png](attachment:15c41678-b2b5-41e1-9bb4-b01c9935f78c:image.png)
    
    ![image.png](attachment:0bf39277-4569-4874-8396-0d07838cf505:image.png)
    
    - Encoder 와 Decoder를 층으로 쌓아서, 각 층에서 어떤 토큰을 집중적으로 봐야하는지에 대한 Self-Attention 스코어 계산
    - 입력 토큰을 한 번에 밀어 넣으면
        - Encoder는 각 토큰을 다른 토큰들과 비교하여, 이 토큰은 얼마나 집중되어야 하는지를 계산
        - Decoder는 Encoder의 계산결과를 바탕으로 토큰을 하나씩 가려가며 해당 토큰을 예측
- 아직까지 Transformer보다 더 나은 구조는 나오지 않았다.
    - 현대 LLM은 대부분 이 구조에서 차용됨 (뒤에도 나오지만, 특히 Decoder 구조에서 많은 영감을 얻음)
    - 학습하기 위해 엄청나게 많은 데이터와 계산이 필요하다.
    - Self-Attention의 계산 복잡도는 $O(N^2)$ : 한 토큰과 나머지 토큰을 행렬 곱을 통해 비교
    - → **병렬처리**가 가능하다는 장점으로 이를 상쇄 중

- Transformer의 세부 원리가 궁금하신 분들만
    
    ![인코더/디코더를 2층으로 줄여 시각화 한 Transformer 모델](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/7e33ca95-f908-4b9c-a1bc-c202f6725226/Untitled.png)
    
    인코더/디코더를 2층으로 줄여 시각화 한 Transformer 모델
    
    ## Embedding & Positional encoding
    
    <aside>
    💡 단어들의 순서를 고려하기 위해 시퀀스의 순서를 벡터로 임베딩하기
    
    </aside>
    
    - Seq2Seq에서는 RNN을 활용하기 때문에 단어가 순서대로 학습된다.
    - Transformer는 PE(Positional Encoding)을 통해 단어 별 위치정보를 나타낸다.
        
        ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/e79987ad-0114-4228-b970-b28e12397dfc/Untitled.png)
        
    - 논문에서는 Input을 512크기의 벡터로 Word Embedding 하였고, 이를 PE 결과 벡터와 합하여 다음 단계로 넘긴다.
    
    ## Multi-head Self-Attention (Encoder Self-Attention)
    
    <aside>
    💡 지시 대명사가 무엇을 가리키고 있는가?, 어떤 단어에 더욱 집중할 것인가? 에 대한 해결 방안
    
    </aside>
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/f92d8d49-42c1-4bac-bf60-363e6239c8e9/Untitled.png)
    
    - 앞에서, 자기 자신의 Attention 스코어를 도출하여 다른 모든 토큰과 비교하여 자신에게 얼마나 집중해야할 지를 알아보는 과정이 Self-Attention이라고 했다.
    - 이러한 Self-Attention 과정을 여러 번 반복하는 것이 Multi-head Self-Attention이다.
    - 굳이 “Multi-head”라고 부르는 이유는 각 head가 병렬적이며, 서로 다른 유형의 정보에 집중하는 방법을 학습할 수 있기 때문이다.
        - 어떤 head는 구문 관계에 주의를 기울일 수도, 어떤 head는 단어 의미의 모호함에 집중할 수도 있다.
    - 3가지 벡터를 활용하여 결과를 도출할 것이다.
        - Query : 물어보는 주체가 되는 벡터
        - Key : 물어보는 대상이 되는 벡터
        - Value : 최종 Attention Value를 계산하기 위한 벡터
    
    - 입력 벡터로 부터  Q, K, V를 생성한다.
    1. Query와 Key값의 내적을 계산한다.
    2. Key 벡터 사이즈로 나눈다.
    3. 2의 결과에 Softmax를 적용한다.
    4. Value와 내적하여 최종 값을 도출한다.
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/bdd974da-9fe2-4327-94de-04a83af49623/Untitled.png)
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/5b6b0597-91bd-470a-b5ca-314f25b68469/Untitled.png)
    
    - 실제로는 위 Self-Attention 과정을 모두 행렬로 변환하여 계산하고, head별로 반복한다.
    - 맨 위 그림과 같이 각 head 별로 Q,K,V 행렬이 계산되고, 이 3개의 행렬을 통해 Attention matrix(Z)를 도출한다.
    - 모든 $Z_n$을 이어붙여서 가중치 행렬 $w_0$와 곱하면 최종적으로 Multi-head Self-Attention의 결과값이 도출된다.
        
        ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b862121f-605d-47a4-be63-a12aa82b34d8/Untitled.png)
        
    
    ## Add & Norm
    
    <aside>
    💡 속도 향상 및 학습 안정을 위한 방법론
    
    </aside>
    
    - ADD : Residual Connection
        - 블록이나 레이어 계산을 건너뛰는 경로를 설정한다.
        - 모델이 다양한 관점에서 블록계산을 수행하며 학습을 쉽게하는 효과가 있다.
        
        ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/065c89b0-c954-4a7e-8855-a1ea7cb14814/Untitled.png)
        
    - Norm : Layer Normalization
        - 미니배치의 인스턴스별 평균과 표준편차를 통해 정규화를 수행한다.
        - 학습 안정 및 속도향상의 효과가 있다.
    
    ## Position-wise FFNN(Feed Forward Neural Networks)
    
    <aside>
    💡 성능을 향상시키기 위한 비선형성의 도입
    
    </aside>
    
    - 비선형성을 도입시켜 복잡한 패턴을 학습할 수 있도록 한다.
    - 두 개의 선형 변환과 그 사이에 비선형 활성화 함수 (ReLU 등)로 구성된다.
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/1ec0295c-903b-4cb5-a6a8-3b5826481600/Untitled.png)
    
    ## Masked Multi-head Self-Attention (Masked Decoder Self-Attention)
    
    <aside>
    💡 현재 위치를 기준으로, 이전 위치 토큰들에 더욱 집중하기 위해 알 수 없는 정보(다음 위치들)는 모두 마스킹 하여 토큰을 예측하기
    
    </aside>
    
    - Seq2Seq 모델에서는 RNN이 순차적으로 단어를 받기 때문에, 현재 위치 기준 뒤에 있는 정보는 알 수 없다.
    - Transformer에서는 Masking을 활용하여 다음 위치들 정보를 가리고, 가리지 않은 부분에 더욱 집중할 수 있도록 한다.
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/0f2f011c-8429-4362-abe7-7044cf223bf2/Untitled.png)
    
    ## Multi-head Attention (Encoder-Decoder Self-Attention)
    
    <aside>
    💡 디코더에서 인코더의 어텐션을  받아, 관련성이 높은 Input에 더욱 집중하도록 하는 방법론
    
    </aside>
    
    - Multi-head Self Attention과 비슷하지만, K,V 벡터를 인코더에서 가져온다는 특징이 있다.
    
    ![](https://nlpinkorean.github.io/images/transformer/transformer_decoding_1.gif)
    
    ## Dense(Linear) & Softmax
    
    <aside>
    💡 디코더의 결과를 사전 크기의 벡터로 투영하고, 그 벡터에서 최종 결과물을 고르는 작업
    
    </aside>
    
    - 다중 인코더와 디코더를 거친 결과는 소수로 이루어진 벡터 하나이다.
    - 결과 벡터를 Linear Layer(fully-connect neural networks)에 통과 시키면, 기존 결과 벡터보다 훨씬 큰 Vocabulary Size의 logits 벡터로 투영된다. 이를 다시 Softmax하면 각 단어에 대한 확률이 도출될텐데, 여기서 가장 높은것을 고르면 최종 결과물을 출력하게 된다.
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/75a77e7e-eadd-4f7c-ae22-b61df255f645/Untitled.png)
    
    ## Summarizing the types of Self-Attention
    
    - 위에서 총 3가지 타입의 Self-Attention이 쓰였다.
        
        ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/552dd81d-c90d-4895-a608-5841a969ae0d/Untitled.png)
        
    - 요약하자면 다음과 같다.
        - Encoder Self-Attention
            - 이해가 주 목적이며, 단어 간 관계를 파악한다.
        - Masked Decoder Self-Attention
            - 예측이 주 목적이며, 다음 단어를 가리고 이전 단어들과의 관계를 사용한다.
        - Encoder-Decoder Attention
            - Encoder의 이해 정보를 통해 Decoder가 다음 단어에 활용한다.
            

### GPT(Generative Pre-trained Transformer) vs. BERT(**Bidirectional Encoder Representations from Transformers)**

- Transformer는 아키텍처일 뿐, 실제 활용 가능한 모델을 만들어내는 것은 다른 과제이다.
- 특히나 Transformer는 굉장히 거대한 아키텍처이기 때문에 사전 학습(Pre-train)이 필수로 여겨진다.
    - 정확히는 아키텍처의 스케일에 비해, 라벨링된 고오급 데이터가 턱없이 부족하기 때문에
    - 라벨링 되어지지 않은 데이터를 대량으로 때려박아 사전 학습 시킨다.
    - 사전 학습 + 작업별 파인튜닝(Fine-tuning) 이라는 대전제를 깔고, 다양한 몯

- BERT(Bidirectional Encoder Representations from Transformers)
    - 구글에서 고안한 구조로, Transformer Encoder 구조에서 착안
    - 양방향(Bidirectional) 학습
        - **MLM(Masked Language Modeling)**: 토큰 15% 가려 놓고 복원
        - **NSP(Next Sentence Prediction)**: 두 문장이 연속인지 분류
    - 문장을 양방향으로 학습하기 때문에 특정 태스크에 굉장히 강력함
        - 문서 분류나 질의 응답, 품사 태깅이나 감정 분류 등
        - 근데 ‘빈칸 채우기’ 형태로 학습하다보니, 생성에는 약하다.

- GPT(Generative Pre-trained Transformer)
    - OpenAI에서 고안한 구조로, Transformer Decoder 구조에서 착안
    - 단방향 학습
        - 오로지 ‘다음 토큰 예측’ 만 죽어라 시킴
        - 상대적으로 계산이 가벼운 Decoder만 있기 때문
        - 단방향으로 학습하기 때문에, 오히려 결과가 자연스러움 (텍스트는 결국 시작과 끝이 있기 때문)
    - 생성을 연습하기 때문에 생성에만 최적화됨
        - 하지만 생성으로 할 수 있는 일은 생각보다 많았다…
        - 프롬프트를 통해 대부분의 태스크를 생성으로 바꾸어 수행할 수 있음

- 모두가 알다시피 최종 승자는 GPT
    - LLM 시장에선 GPT 구조가 이미 점령함
    - (그렇다고 BERT가 안쓰이는 것은 아닙니다! 경량화해서 특정 태스크에서 아주 좋은 성능을 보이는 중)
    - 많은 도전들이 있었지만, 결국 벌크업한 GPT가 다 이겨먹고 있는 중
    

## 대 LLM의 시대

### 규모의 경제가 적용되는 LLM

- GPT의 등장부터 지금까지는, 규모의 경제가 아주 잘 동작하는 생태계였다.
- 파라미터만 늘리면 성능이 기하급수적으로 올랐다.
    - 아까 그 숫자 하나가 파라미터 하나라고 했는데
    - 요즘은 경량 모델이라고 부르는 것이 기본 백만개의 파라미터를 갖는다…
- 스케일링 법칙(Scaling Laws)이 발견됨
    - 모델 크기, 데이터 크기, 컴퓨팅 자원이 증가하면 성능이 예측 가능하게 향상
    - 이론적으로는 더 큰 모델이 항상 더 좋은 성능을 보임
    - GPT-1 (2018): 117M 파라미터
    GPT-2 (2019): 1.5B 파라미터  
    GPT-3 (2020): 175B 파라미터
    GPT-4 (2023): (추정) 1.76T 파라미터
    - 파라미터가 10배씩 증가할 때마다 성능이 급격히 향상됨
    - 하지만 세상이 녹록치 않듯, 이 스케일링 법칙이 깨졌기 때문에 GPT-5가 못나오고 있다는 소문이 돈다.
- RLHF(Reinforcement Learning from Human Feedback)의 등장
    - GPT-3까지는 그냥 "다음 토큰 예측"만 했는데, 사람이 원하는 답변과는 거리가 멀었음
    - 사람의 피드백을 통해 모델을 조정하는 기법이 도입됨
    - ChatGPT의 핵심 기술 중 하나
    - how-to?
        
        1. **SFT(Supervised Fine-Tuning):** 고품질 대화 데이터로 파인튜닝
        
        2. **Reward Model 학습:** 사람의 선호도를 학습하는 모델 생성
        
        3. **PPO(Proximal Policy Optimization):** 보상 모델을 활용해 정책 최적화
        
    - 그 결과 ChatGPT(GPT-3.5)는 GPT-3보다 파라미터는 적지만 훨씬 유용했다
        - 단순히 크기만이 능사가 아니라는 것을 보여줌
        - 어떻게 학습시키느냐가 또한 굉장히 중요한 요소

### 언어모델은 결국 함수

- 위에서 언급했듯, **언어모델은 결국 확률적 함수** (모든 AI가 그러함)
    - 사용자들이 LLM을 활용할 때 자주 겪는 문제들의 근본 원인
    - "왜 이상한 답변을 하지?" "왜 때로는 거짓말을 하지?" "왜 프롬프트에 이렇게 민감하지?"
    - 입력 토큰 시퀀스 → 다음 토큰의 확률 분포
    - 실제로는 의미를 "이해"하는 것이 아니라, 패턴을 학습한 것
    - "서울의 수도는?"이라는 질문에 "대한민국"이라고 답하는 이유
    → 학습 데이터에서 그런 패턴을 많이 봤기 때문
    → 진짜로 서울과 대한민국의 관계를 이해해서가 아님

- 왜 믿을 수 없는가? - 할루시네이션(Hallucination)의 원리
    - 모델은 항상 "뭔가"를 출력해야 함 (확률 분포에서 샘플링)
    - 확실하지 않은 정보도 그럴듯하게 만들어냄
    
    ![image.png](attachment:721ef4d3-ec45-4aa5-a87f-62dd0e00cbe5:image.png)
    

- 결국 큰 모델을 제어할 수 있는건 프롬프트 뿐
    - 같은 의미라도 표현 방식에 따라 모델이 활성화하는 패턴이 달라짐
    - "계산해줘" vs "단계별로 차근차근 계산해줘" → 결과가 다름
    - 이는 학습 데이터에서 어떤 맥락과 함께 나타났는지에 영향받음

### 신뢰할 수 있는 결과를 위한 다양한 노력들

- Chain of Thought (CoT)
    - 복잡한 문제를 바로 입력으로 집어넣으면, 의도치 않은 결과가 튀어나올 때가 너무나 많음
    - 프롬프트를 자세히 풀거나, ‘차근차근 단계별로 생각해보자’ 라는 뉘앙스를 넣었더니 성능이 상승함
    - 왜 효과적인가? 
    → 학습 데이터에서 단계별 풀이가 포함된 텍스트들이 정확도가 높았음
    → 모델이 해당 패턴을 따라하게 됨
    
    ```tsx
    (X) 이 버그 해결해줘
    (O) 이런 버그가 여기서 터졌어. 문제를 차근차근 분석하고, 해결하기 위해 단계별로 접근해줘
    ```
    
    - 근데 요즘은 Reasoning 모델이 나와서, 요런 과정을 알아서 다 해준다.
        - 각 모델 Provider가 이 메커니즘을 공개하지는 않지만, 원리 자체는 CoT와 동일
        - 커서에 있는 요 뇌 아이콘 있는 모델이나, thinking 이라는 접미어가 붙으면 대부분 Reasoning 모델입니다!
        - 대부분의 경우 속도를 희생하고 좀 더 신뢰도 있는 결과를 얻을 수 있으나, 오히려 자가당착에 빠지는 경우도 드물지만 있음
        
        ![image.png](attachment:d3eec654-72e7-4e26-98b4-1e15ab0a956f:image.png)
        

- RAG(Retrieval-Augmented Generation)
    - LLM의 사전 학습은 상당히 오랜 시간이 걸리기 때문에, cut-off 개념이 존재함
    
    ![image.png](attachment:0dce4b3b-98ec-406f-814c-c647eb471e5f:image.png)
    
    - 그래서 최신 정보나 필요한 컨텍스트에 특화된 정보를 LLM이 검색할 수 있게 벡터 DB에 미리 넣어둠
    1. 사용자 질문이 들어옴
    2. 질문과 관련된 문서를 벡터 DB에서 검색
    3. 검색된 문서 + 원래 질문을 함께 LLM에 전달
    4. LLM이 검색된 정보를 바탕으로 답변 생성

- Tool Use & Function Calling, MCP(Model Context Protocol)
    - RAG의 연장선 상으로, LLM에게 다양한 도구를 쥐어주자는 메커니즘
    - 다양한 도구와 그 설명을 LLM에게 제공하면, 필요한 도구를 스스로 선택하고 호출할 수 있게 함
    - 유출되는 LLM의 시스템 프롬프트들을 보면, 도구에 대한 설명이 적혀있음
    https://github.com/jujumilk3/leaked-system-prompts/blob/main/openai-chatgpt4o-20250506.md
    - MCP: Anthropic에서 제안한, LLM의 도구 사용에 대한 새로운 표준
    - 기존에는 각 도구마다 다른 방식으로 연결해야 했는데, MCP로 표준화

### 가성비와 속도를 위한 다양한 노력들

- 모델 크기의 딜레마
    - 큰 모델 = 좋은 성능, 하지만 느리고 비쌈
    - 작은 모델 = 빠르고 저렴, 하지만 성능 제한
    - 실제 서비스에서는 속도와 비용이 중요함
    - 그래서 대부분의 LLM은 파라미터 크기 별로 쪼개져서 나옴
        - Claude의 Opus, Sonnet, Haiku
        - Google Gemini의 Flash-lite, Flash, Pro
        - Meta Llama는 사이즈를 그대로 표기 (8B, 13B, 70B …)

- 양자화(Quantization)
    - 모델 파라미터가 32bit 실수로 저장되어 용량이 큼
    - 그래서 타입을 바꿔(!) 저장한다.
    - 어느정도 성능 손실은 있겠지만, 모델 크기와 런타임 메모리 사용량이 대폭 감소
    - Llama-3-8B 모델을 기준으로
      FP32: ~32GB
      FP16: ~16GB  
      INT8: ~8GB
      INT4: ~4GB

- MoE (Mixture of Experts) 아키텍처
    - 모든 파라미터를 다 쓰는게 아니라, 상황에 맞는 "전문가"들만 깨워서 일시키는 구조
    - 예를 들어 GPT-4는 8개의 전문가가 있는데, 질문에 따라 그 중 2개만 활성화됨
        - 수학 문제가 들어오면 → 수학 전문가 + 논리 전문가 활성화
        - 코딩 문제가 들어오면 → 프로그래밍 전문가 + 문법 전문가 활성화
    - 결과적으로 큰 모델의 성능을 유지하면서도 실제 계산량은 작은 모델 수준
    - 그래서 GPT-4가 1.76T 파라미터라고 해도, 실제로는 훨씬 적게 사용됨

- 지식 증류(Knowledge Distillation)
    - 큰 모델이 작은 모델한테 "이렇게 답변해야 해"라고 가르치는 방식
    - 마치 베테랑 개발자가 신입에게 코드 리뷰해주는 것과 비슷
        - Teacher(큰 모델): "이 문제는 이런 식으로 접근해야 해"
        - Student(작은 모델): "아 그렇구나!" 하고 학습
    - ChatGPT-4o-mini가 대표적인 예시
        - GPT-4의 답변 스타일과 지식을 학습했지만 훨씬 가벼움
        - 성능은 GPT-3.5보다 좋으면서도 속도는 몇 배 빠름
        - 비용도 훨씬 저렴해서 실제 서비스에서 많이 활용됨

- 심지어 요즘엔 이미지처리 메커니즘을 들고와 연구중이다.
    - 많이들 들어보셨을 Stable Diffusion에 쓰이는 그 Diffusion 구조를 LLM에 차용
    - (생성 속도가 충격적…)

[Gemini Diffusion](https://deepmind.google/models/gemini-diffusion/)

## 번외) 그래서 지금 무슨 모델을 써야할까?

[LMArena](https://lmarena.ai/)

[WebDev Arena](https://web.lmarena.ai/)